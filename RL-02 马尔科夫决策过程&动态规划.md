# RL-02 马尔科夫决策过程&动态规划

*Arthur  2020 / 21 / 05*

[toc]



## RL基本概念

![img](RL-02 马尔科夫决策过程&动态规划.assets/rl_concept.png)

根据上图, 任务的目标就是: 获取尽可能多的 $Reward$ . 每个时间片, 智能体根据当前的状态, 来确定下一步的动作, 即需要一个 $state$ 找出一个 $action$ , 使得 $reward$ 最大. 这里的从 $state$ 到 $action$ 的过程就称之为一个策略 $Policy$, 一般用 $\pi$ 表示. 

 **RL的任务就是找一个最优的策略 $ Policy $ 从而使 $ Reward $ 最多.**

在运行RL的算法之前, 首先需要得到一系列的 $state, action, reword $ , 这个过程通常是智能体随机得到的, 是一系列的样本, 如下:

$$(s_1, a_1, r_1, s_2, a_2, r_2, \dots, s_t, a_t, r_t )$$

RL的算法就是根据这些样本来改进 $Policy$ , 从而使得样本中的 $Reward$ 更好, 这种算法具有让 $Reward$ 越来越好的特性, 因此被称为强化学习(Reinforcement Learning).



## 马尔科夫链 ( Markov Chain)

- 马尔科夫特性： 只依赖当前状态来预测下一个状态，而与之前的状态无关。
- 马尔科夫表： 用表的形式来表示状态转移到下一个状态的转移概率。

| 当前状态 | 下一个状态 | 转移概率 |
| -------- | ---------- | -------- |
| 多云     | 下雨       | 0.6      |
| 下雨     | 下雨       | 0.2      |
| 晴天     | 多云       | 0.1      |
| 下雨     | 晴天       | 0.1      |



## 马尔科夫决策过程 ( Markov Decision Process)

- 马尔科夫决策过程实际上是对环境的建模, 它与马尔科夫链的区别是加入了 $action$ 和 $rewards$ 的概念: 
- 一个基本的MDP可以用一个五元组来表示 ( $S, A, P, R, \gamma $ ):
  - 智能体所处的一组状态（S）
  - 智能体从一组状态转移到另一组状态所执行的一组行为（A）
  - 转移概率（$P_{ss}^{a}$）, 执行某一个行为 $a$ , 从一个状态 $s$ 转移到另一个状态 $s^{'}$  的概率
  - 奖励概率（$R_{ss^{''}}^a$）, 执行某一行为 $a$ , 从一个状态 $s$ 转移到另一个状态 $s^{'}$ 所获得奖励的概率
  - 折扣因子 ($\gamma$) , 控制及时奖励和未来奖励的重要性, fanwei  [0,1] 
- 注: MDP的核心问题, 就是找一个策略 $\pi$ , 来决定在状态 $s$ 下选择哪个动作;  这样, MDP就变成了一个马尔科夫链.



## 奖励

- 智能体追求从环境中获得的总奖励(累计奖励)最大化,而不仅仅是及时奖励.

  $R_t =  r_{t+1} + r_{t+2} + \dots + r_{T}$

- 加入折扣因子之后的奖励和回报:

  $R_t =  r_{t+1} + \gamma r_{t+2} + \gamma^{2} r_{t+2} + \gamma^{3} r_{t+} + \dots + \gamma^{T-(t+1)} r_{T} = \Sigma_{k=0}^{T} \lambda^{k}r_{t+1+k}$

- 若折扣因子为0, 则指挥考虑即时奖励;  若为1, 则会更考虑未来奖励;



## 策略函数

- 策略函数: 策略代表了为达成目标而选择执行的行为, 是从状态到行为的映射, 表示每个状态执行什么行为, 记为 $\pi$:

  $\pi(s): S \rightarrow A $

- 对于一个给定的状态, 所有可能动作的概率函数求和/积分为1:

  $\Sigma_a \pi(s, a) = 1$

- 最终目标是找到一个最优策略,在每个状态都指定正确的行为,从而使奖励最大化;

  ![策略](RL-02 马尔科夫决策过程&动态规划.assets/rl_policy.png)

  如上图,当处于”Hungry”状态是, 有两种行为“Eat”或”Don’t Eat”. 朴素的观察, 发现最优策略是”Eat”, 因此此时的最优策略 $\pi^\star$ 可以记作: 

  $\pi^\star (Hungry) = Eat $ 



## 状态值函数

- 简称为值函数, 确定一个智能体在策略 $\pi$ 下处于某一特定状态的最佳程度, 记为 $V(s)$ 表示执行策略后状态的值,  它等价于智能体从状态 $s$ 开始, 经历多个时间步到达终结状态, 所累积获得的奖励期望;

- 在策略 $\pi$ 下, 从状态 $s$ 开始获得的期望回报:

  $V^{\pi}(s) = E_\pi[ R_t | s_t = s ] = E_\pi[\Sigma_{k=0} \gamma^{k} r_{t+1+k} | s_t = s] = E_\pi [ R_t + \gamma V^\pi(s_{t+1}) | s_{t} = s]$
  
  注: 以上公式最后部分, 就是Bellman方程基本形态,  体现的是当前状态的价值和下一步的价值的迭代关系;
  
- 值表: 用表的形式表示不同状态的值(期望回报):

  | 状态  | 值(期望回报) |
  | ----- | ------------ |
  | 状态1 | 0.3          |
  | 状态2 | 0.9          |

  由上表可以得知, 状态2的值较大, 因此状态2优于状态1 , 这意味着当某个状态可以向状态1或2转移时, 转移到状态2将获得较大的回报. 



## 状态-行为值函数

- 也称之为Q函数, 用于表示智能体执行策略 $\pi$ , 在某一个状态执行某一个特定行为 $a$ 的最佳程度, 本质就是回报值. 记为 $Q(s)$ ;

- 根据策略 $\pi$ , 在状态 $s$ 采取行为 $a$ 后,  直到终结状态,  期间多个时间步, 所累积获得的期望回报如下:

  $ Q^\pi (s,a) = E_\pi [ R_t | s_t = s, a_t = a  ] = E_\pi[ \Sigma_{k=0} \gamma^k r_{r_t+1+k} | s_t=s, a_t=a ]  $

- Q表: 用表的形式表示状态下不同行为的值(期望回报)

  | 状态      | 行为      | 值(期望回报) |
  | --------- | --------- | ------------ |
  | **状态1** | **行为1** | **0.03**     |
  | 状态1     | 行为2     | 0.02         |
  | 状态2     | 行为1     | 0.5          |
  | **状态2** | **行为2** | **0.9**      |

  由上表可以得知, 状态1时执行行为1, 状态2时执行行为2具有较大的值(回报), 这意味着当智能体处于状态1或2时, 选择这些行为将获得较大的回报. 



## 状态值函数（值函数）vs 状态－行为值函数（Ｑ函数）

- 值函数：确定的是: 遵循某个策略 $\pi$ , 状态 $s$ 的最佳程度. 此时不涉及行为, 由策略 $\pi$ 决定采取什么行为; 

- Q函数：确定的是: 遵循某個策略 $\pi$ , 在状态 $s$ 下采取行为的最佳程度. 即评估不同的行为 $a$ 的期望回报;

  ![策略](RL-02 马尔科夫决策过程&动态规划.assets/rl_policy.png)

  如上图,当处于”Hungry”状态是, 有两种行为“Eat”或”Don’t Eat”. 分别计算这两个行为的单步回报:

  $R_{Eat} = 0.9*1 + 0.1*(-1) = 0.9 $

  $R_{Don't Eat} = 0.9*(-1) + 0.1*(-10) = -1.9 $
  
  因此:
  
  $\pi^\star(Hungry) = Eat$
  
  $V^\star(Hungry) = 0.9$



## 小结:

- 由以上可以看出, MDP问题中, 决定一个智能体的期望回报的, 主要是由上面所说的策略 $\pi$ 和值函数 $V(s)$ 以及Q函数 $Q(s)$ 决定的, 所以, 求解MDP时, 就是寻求最优策略和值函数/Q函数, 最优值函数记为  $V^\star(s)$ :

  $ V^\star(s) = \max_\pi V^\pi (s)$

  由于Q函数是针对一个状态的多个可能行为的回报的表示, 因此Q函数的最优值, 也是值函数的最优值:

  $V\star(s) = \max_\pi Q^\star (s,a)$



## Bellman方程

- 定义符号 $\Rho$  , 根据前面的价值函数, 遍历所有的行为 $a$,  针对每个行为 $a$, 遍历所有可能的下一状态 $s'$ ,计算价值函数, 然后求和, 可以得到价值函数 $V^\pi (s) $  的递推公式如下:

  $V^\pi (s)= \sum_a \pi(s,a) \sum_{s'} \Rho_{ss'}^a [ R_{ss'}^a + \gamma V^\pi (s')]$

  其中:

  - $a$ 代表在状态 $s$ 下, 可以选择的行为;
  -  $\Rho_{ss'}^a$ 代表的是在状态 $s$ 下, 选择行为 $a$ 进入到状态 $s'$  的概率; 
  - $R_{ss'}^a$ 代表的是在状态 $s$ 下, 选择行为 $a$ 后所得到的回报;
  - $\gamma$ 代表的是折扣因子;

- 同理的, Q函数进行类比, 针对已知的行为 $a$, 遍历所有可能的下一状态 $s'$ , 计算Q函数值,  然后求和, 可以得到Q函数 $Q^\pi (s,a)$ 的递推公式如下:

  $Q^\pi (s,a) = \sum_{s'} \Rho_{ss'}^a [ R_{ss'}^a + \gamma \sum_{a'} Q^\pi(s', a') ] $

  其中: 

  -  $\Rho_{ss'}^a$ 代表的是在状态 $s$ 下, 选择行为 $a$ 进入到状态 $s'$  的概率; 
  - $R_{ss'}^a$ 代表的是在状态 $s$ 下, 选择行为 $a$ 后所得到的回报;
  - $\gamma$ 代表的是折扣因子;
  - $a'$ 代表在状态 $s'$ 下, 可以选择的所有的行为;

- 以上两个公式合并( 后者代入前者 ), 可以得到基于Q函数的价值函数的的最优化解 $V^\star(s)$ 为如下问题, 称之为Bellman最优方程:

  $V^\pi (s) = \max_ａ \sum_{s'} \Rho_{ss'}^a [ R_{ss'}^a ＋　\gamma \sum_{a'} Q^\pi (s',a')]  $



## 动态规划

### 1. 基本概念

- **动态规划(DP)** : 在动态规划中, 并不是直接求解复杂问题, 而是将问题分解成简单的子问题, 然后对每个子问题计算并保存该解. 如果出现相同的子问题, 则不再重新计算, 而是使用已求得的解.
- 两种算法:
  - 值迭代
  - 策略迭代

### 2. 值迭代

- 首先是从一个随机值函数 $V(s) $ 开始, 比如设置为0等, 然后根据这些初始的状态的值, 来计算 $Q(s,a)$ 
- 然后用计算得到的最大的 $Q(s,a)$ 来刷新 $V(s)$ 
- 然后用刷新后的 $V(s)$ 计算 $Q(s,a)$, 不断的进行上述步骤, 直到 $V(s)$ 最优 ( 两次值函数之间的变化很小 )

```flow
st=>start: 开始
op1=>operation: 初始化随机值函数 V(s)
op2=>operation: 对于每个状态, 计算 Q(s,a)
op3=>operation: 因为 V(s) = Max Q(s,a), 
用Q(s,a)的最大值更新值函数V(s)
cond=>condition: V(s)最优?
e=>end
st->op1(right)->op2->op3(right)->cond
cond(yes)->e
cond(no)->op2

```

例子:  

- 一开始, 所有的状态 $s$ 的值都是, 即 $V(s_A) = V(s_B) = \dots = 0 $ 
- 假设知道初始状态 $s_A$ 到其他状态的转移概率和奖励( 很多时候转移概率也未知, 且奖励也未知 ) 
- 由已知信息, 可以计算出 $Q(s_A,a_1), Q(s_A,a_2), \dots , Q(s_A, a_k)$ , 即每个行为 $a_k$ 的Q值
- 找到其中最大的, 假设为 $Q(s_A,a_m)$ , 用  $ Q(s_A,a_m) $ 作为状态 $s_A$ 的值, 即 $V(s_A) = Q(s_A,a_m)$
- 采用相同的方式, 可以更新其他状态的值 $V(S_i)$ , 把所有的状态都更新一遍, 算作一轮;
- 如此迭代多轮, 直到两轮之间的 $V(S)$ 变化已经很小时,就停止迭代;

### 3. 策略迭代

- 首先是从一个随机策略 $\pi$ 开始, 这样, 针对每个状态, 都有一些针对的行为, 然后基于这些行为, 来计算 值函数 $ V(s) $ 方法和值迭代类似, 就是用新的状态的值, 来计算 $Q(s,a) $;
- 然后用计算得到的最大的 $Q(s,a)$ 来刷新 $V(s)$ 
- 基于新的值函数 $V(s)$ , 采用新的策略: 即每个状态取具有最大值的行为 , 然后用这个策略, 计算 $Q(s,a)$ 
- 不断的进行上述步骤, 直到 $V(s)$ 最优 ( 两次值函数之间的变化很小 )

```flow
st=>start: 开始
op1=>operation: 初始化策略 π
op2=>operation: 根据策略, 对于每个状态, 计算 Q(s,a)
op3=>operation: 因为 V(s) = Max Q(s,a), 
用Q(s,a)的最大值更新值函数V(s)
op4=>operation: 找到改进的策略
(取最大值的行为)
cond=>condition: V(s)最优?
e=>end
st->op1(right)->op2->op3(right)->op4->cond
cond(yes)->e
cond(no)->op2
```

例子:

- 一开始, 初始化一个随机策略, 例如让每个状态随机的选择一个行为作为初始化策略,  $\pi({s_A})=a_1, \pi({s_B})=a_2,\dots $
- 使用这些策略, 计算这些状态的值函数 $V(s)$
- 根据值函数 $V(s)$ 计算 $Q(s,a)$ 
- 找到改进的策略:  选择每个状态中使得 $Q(s,a)$ 最大的那个作为新的策略
- 然后重复以上过程, 直到 $V(s)$ 最优 ( 两次值函数之间的变化很小 )



## 例子

### 1. 题目

|  S   |  F   |  F   |  F   |
| :--: | :--: | :--: | :--: |
|  F   |  H   |  F   |  H   |
|  F   |  F   |  F   |  H   |
|  H   |  F   |  F   |  G   |

如上图所示:

- $\rm{S}$ 是起始位置, $\rm F$ 是可通过的节点,  $\rm H$ 是陷阱, $\rm  G$ 是目标; 
- 从 $S$ 出发, 上下左右移动, 到达 $G$ 即为结束;
- 智能体的目标是找到从 $S$ 到 $G$ 的最佳路径且不会陷入H ;
- 奖励: 正确行走一步, +1奖励;  掉入洞中或不移动, 奖励为0 ;

### 2. MDP问题构建

- MDP包含如下内容:
  - **状态:** 状态集. 上述网格中的4x4总共16个格子, 因此有16个状态;
  - **行为:** 所有的可能的行为集合(上下左右). 
  - **转移概率:** 执行行为 $a$ , 从一个状态转移到另一个状态的概率;
  - **奖励概率:** 执行行为 $a$ , 从一个状态转移到另一个状态获得奖励的概率;

- 3个函数: 最优的意思就是实现奖励最大化的程度
  - **策略函数:** 指定每个状态下所执行的行为
  - **值函数:** 指定状态的最优程度
  - **$Q$ 函数:** 指定某一个状态下行为的最优程度

### 3. 值迭代

使用值迭代算法.

```python
import gym
import numpy as np

# build the environment
gEnv = gym.make("FrozenLake-v0")
gEnv.reset()
print("show env:")
gEnv.render()

# show the environment size: 4*4=16
print("env space size: ", gEnv.observation_space.n)
# show the action space size: 上下左右
print("action space size: ", gEnv.action_space.n)


# 值迭代过程:  本质是状态V值的迭代刷新过程
#   1. 先初始化一个为0的 V值表 V(s)
#   2. 根据V(s) 和 从env获取的所有action得到的rewards, 计算出 Q(s,a)
#   3. 通过 maxQ(s,s) 刷新 V(s)
#   4. 不断重复以上过程2, 3过程, 直到V(s)刷新前后差别很小时, 退出迭代过程
def value_iteration(env, gamma=1.0):
    env = env.unwrapped
    # 初始化 V(s) 为全0
    value_table = np.zeros(env.observation_space.n)
    # 最高迭代10^5次
    no_of_iterations = 10 ** 5
    # 迭代退出条件: V(s)刷新前后的差值<=1^(-20)
    threshold = 1e-5

    policy_table = np.zeros(env.observation_space.n)
    # 开始迭代
    for i in range(no_of_iterations):
        # 保存原有的 V(s), 以便于最后求V(s)更新前后的差值
        pre_value_table = np.copy(value_table)
        # 遍历所有的状态, 以便于更新每个状态对应的V值
        for state in range(env.observation_space.n):
            # 由于某个状态的V值本质是最大的Q(s,a),因此用此Q值表保存所有的动作a对应的Q值
            # 由于一个动作对应的下一个状态可能更会有多,需要求和得到本状态下, 执行本动作后, 进入所有可能的下一个状态时获得的奖励总和
            Q_table = np.zeros(env.action_space.n)
            # 遍历所有的动作, 求出动作action对应Q值, 添加到Q值表中
            for action in range(env.action_space.n):
                # 针对本状态执行action后所有的可能下一个状态,都计算其 转移概率, 奖励
                for next_sr in env.P[state][action]:
                    # 转移概率trans_prob, 奖励 reward
                    trans_prob, next_state, reward, _ = next_sr
                    # 按照公式求奖励:
                    #     Q(s,a) = 转移概率 * ( 本此动作的奖励 + gamma * 下一个状态的V值)
                    action_reward = trans_prob * (reward + gamma * value_table[next_state])
                    # 把本次状态转移的奖励添加到总奖励中
                    Q_table[action] += action_reward
            # 在本状态的所有动作中挑出最大的奖励, 作为本状态新的V值
            value_table[state] = np.max(Q_table)
            # 从Q值表中选出最大的那个动作(np.argmax获得序号,且序号为action), 作为本状态的新的Policy
            policy_table[state] = np.argmax(Q_table)

        # 求两次迭代之间的V值表差值
        delta = np.sum(np.fabs(value_table - pre_value_table))
        # 若达到迭代退出条件, 则退出
        if delta <= threshold:
            print("converged at iteration #%d." % (i + 1))
            break
    # 返回刷新后的V值表和策略表
    return value_table, policy_table


v_table, p_table = value_iteration(env=gEnv, gamma=1.0)

print("value_table: \n", v_table)
print("policy_table: \n", p_table)
```

输出结果:

```shell
show env:
SFFF
FHFH
FFFH
HFFG
env space size:  16
action space size:  4
converged at iteration #305.
value_table: 
 [0.82349991 0.82349023 0.8234835  0.82348008 0.82350288 0.
 0.52939165 0.         0.82350752 0.82351348 0.76469192 0.
 0.         0.88234187 0.94117084 0.        ]
policy_table: 
 [0. 3. 3. 3. 0. 0. 0. 0. 3. 1. 0. 0. 0. 2. 1. 0.]
```

