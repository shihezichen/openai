# RL-03 蒙特卡罗方法

*Arthur   May 05, 2020*

[toc]

## 时间差分学习

- **简介**

  基于先前的学习的估计值来近似当前估计值， 也成为自举（bootstrapping）。蒙特卡罗方法没有自居，只能在情景结束时才能进行估计。

- **时间差分（TD）更新规则：**

  时间差分学习采用TD更新的规则来更新一个状态的值：

  $V(s) = V(s) + \alpha( r + \gamma V(s') - V(s) )$

  上式本质上 $r + \gamma V(s')$ 与  $V(s)$ 的偏差， 称为时间差分误差， 经过多次迭代， 试图使该误差最小化。其中的 $\alpha$ 称为学习率。

## 时间差分控制

- 简介

  在时间差分预测中， 是估计值函数。 在时间差分控制中， 是优化值函数。有两种控制算法：

  - 离线策略学习算法:  Q学习
  - 在线策略学习算法： SARSA

### Q学习

- Q学习中根据下列方程更新Q值：

  $ Q(s,a) = Q(s,a) + \alpha [r + \gamma maxQ(s',a') - Q(s,a) ]$

  例子： 以冰湖为例，假设当前状态处于(3,2)， 并且具有两种行为（向左或向右），已知的Q值表如图所示。

  ![image-20200705173224465](RL-04 时间差分学习.assets/IceLake例子.png)

  设选择某一个概率 $\epsilon$ 并探索一中心的向下的行为，到达行的状态（4，2），该如何更新（3，2）的值？ 

  设 $\alpha$ 为0.1， 折扣因子 $\gamma$ 为1， $r$ 为0.3, 则代入公式：

  $ Q(s,a) = Q(s,a) + \alpha [r + \gamma maxQ(s',a') - Q(s,a) ]$  

  后为：

  $Q((3,2),向下) = Q((3,2),向下) + 0.1[ 0.3 + 1*maxQ((4,2),a') - Q((3,2),向下)]$

  查表可知 $Q((3,2),向下)$ 的值为0.8, $maxQ((4,2), a’)$ 为$Q((4，2),向右)$ ，其值为0.8

  所以：

  $Q((3,2),向右) = 0.8 + 0.1[0.3+ 1*0.8 - 0.8] = 0.83$

​		

​		同理， 考虑状态 (4,2), 此时可知向右的行为具有最大值， 则如何更新 Q((4,2), 向右)的值， 同上所述：

​		$Q((4,2),向右) = 0.8 + 0.1[0.3+ 1*0.3 - 0.8] = 0.78$



